{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "InferenceServerException",
     "evalue": "[StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:8000: End of TCP stream",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     30\u001b[0m input_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGM to seek court protection against ignition lawsuits\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 31\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43msend_request_to_bert_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m, in \u001b[0;36msend_request_to_bert_classifier\u001b[1;34m(input_string)\u001b[0m\n\u001b[0;32m     15\u001b[0m inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_data_from_numpy(numpy_input)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Send inference request\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert_classifier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Process the output\u001b[39;00m\n\u001b[0;32m     21\u001b[0m output_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mas_numpy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOUTPUT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hoanganh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tritonclient\\grpc\\_client.py:1572\u001b[0m, in \u001b[0;36mInferenceServerClient.infer\u001b[1;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, client_timeout, headers, compression_algorithm, parameters)\u001b[0m\n\u001b[0;32m   1570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n\u001b[1;32m-> 1572\u001b[0m     \u001b[43mraise_error_grpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrpc_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hoanganh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tritonclient\\grpc\\_utils.py:77\u001b[0m, in \u001b[0;36mraise_error_grpc\u001b[1;34m(rpc_error)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_error_grpc\u001b[39m(rpc_error):\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Raise an :py:class:`InferenceServerException` from a gRPC error.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    InferenceServerException\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m get_error_grpc(rpc_error) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInferenceServerException\u001b[0m: [StatusCode.UNAVAILABLE] failed to connect to all addresses; last error: UNAVAILABLE: ipv4:127.0.0.1:8000: End of TCP stream"
     ]
    }
   ],
   "source": [
    "def send_request_to_bert_classifier(input_string):\n",
    "    # Assuming input_strings is a list of strings\n",
    "    # Create a Triton client instance\n",
    "    client = grpcclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "\n",
    "    # Prepare the input data\n",
    "    numpy_input = np.array([bytes(input_string, encoding='utf-8')])\n",
    "\n",
    "    # Define the input tensor\n",
    "    inputs = [\n",
    "        grpcclient.InferInput('CONTENT', numpy_input.shape, \"BYTES\")\n",
    "    ]\n",
    "\n",
    "    # Set the data for the input tensor\n",
    "    inputs[0].set_data_from_numpy(numpy_input)\n",
    "\n",
    "    # Send inference request\n",
    "    response = client.infer(model_name=\"bert_classifier\", inputs=inputs)\n",
    "\n",
    "    # Process the output\n",
    "    output_data = response.as_numpy('OUTPUT')\n",
    "    x = response.as_numpy('PROB')\n",
    "    print(x)\n",
    "    # Assuming the output is also in bytes and needs decoding\n",
    "    decoded_output = [output.decode('utf-8') for output in output_data.flatten()]\n",
    "\n",
    "    return decoded_output\n",
    "\n",
    "# Example usage\n",
    "input_texts = \"GM to seek court protection against ignition lawsuits\"\n",
    "output = send_request_to_bert_classifier(input_texts)\n",
    "print(\"Inference output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"src\\\\gevent\\\\greenlet.py\", line 908, in gevent._gevent_cgreenlet.Greenlet.run\n",
      "  File \"c:\\Users\\hoanganh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tritonclient\\http\\_client.py\", line 1591, in wrapped_post\n",
      "    return self._post(request_uri, request_body, headers, query_params)\n",
      "  File \"c:\\Users\\hoanganh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tritonclient\\http\\_client.py\", line 296, in _post\n",
      "    response = self._client_stub.post(\n",
      "  File \"c:\\Users\\hoanganh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\geventhttpclient\\client.py\", line 272, in post\n",
      "    return self.request(METHOD_POST, request_uri, body=body, headers=headers)\n",
      "  File \"c:\\Users\\hoanganh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\geventhttpclient\\client.py\", line 253, in request\n",
      "    response = HTTPSocketPoolResponse(sock, self._connection_pool,\n",
      "  File \"c:\\Users\\hoanganh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\geventhttpclient\\response.py\", line 292, in __init__\n",
      "    super(HTTPSocketPoolResponse, self).__init__(sock, **kw)\n",
      "  File \"c:\\Users\\hoanganh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\geventhttpclient\\response.py\", line 164, in __init__\n",
      "    self._read_headers()\n",
      "  File \"c:\\Users\\hoanganh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\geventhttpclient\\response.py\", line 184, in _read_headers\n",
      "    data = self._sock.recv(self.block_size)\n",
      "  File \"c:\\Users\\hoanganh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gevent\\_socketcommon.py\", line 666, in recv\n",
      "    self._wait(self._read_event)\n",
      "  File \"src\\\\gevent\\\\_hub_primitives.py\", line 317, in gevent._gevent_c_hub_primitives.wait_on_socket\n",
      "  File \"src\\\\gevent\\\\_hub_primitives.py\", line 322, in gevent._gevent_c_hub_primitives.wait_on_socket\n",
      "  File \"src\\\\gevent\\\\_hub_primitives.py\", line 313, in gevent._gevent_c_hub_primitives._primitive_wait\n",
      "  File \"src\\\\gevent\\\\_hub_primitives.py\", line 314, in gevent._gevent_c_hub_primitives._primitive_wait\n",
      "  File \"src\\\\gevent\\\\_hub_primitives.py\", line 46, in gevent._gevent_c_hub_primitives.WaitOperationsGreenlet.wait\n",
      "  File \"src\\\\gevent\\\\_hub_primitives.py\", line 46, in gevent._gevent_c_hub_primitives.WaitOperationsGreenlet.wait\n",
      "  File \"src\\\\gevent\\\\_hub_primitives.py\", line 55, in gevent._gevent_c_hub_primitives.WaitOperationsGreenlet.wait\n",
      "  File \"src\\\\gevent\\\\_waiter.py\", line 154, in gevent._gevent_c_waiter.Waiter.get\n",
      "  File \"src\\\\gevent\\\\_greenlet_primitives.py\", line 61, in gevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\n",
      "  File \"src\\\\gevent\\\\_greenlet_primitives.py\", line 61, in gevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\n",
      "  File \"src\\\\gevent\\\\_greenlet_primitives.py\", line 65, in gevent._gevent_c_greenlet_primitives.SwitchOutGreenletWithLoop.switch\n",
      "  File \"src\\\\gevent\\\\_gevent_c_greenlet_primitives.pxd\", line 35, in gevent._gevent_c_greenlet_primitives._greenlet_switch\n",
      "socket.timeout: timed out\n",
      "2024-04-25T05:33:50Z <Greenlet at 0x23ee0fff400: wrapped_post('v2/models/bert_classifier/infer', b'{\"inputs\":[{\"name\":\"CONTENT\",\"shape\":[1],\"dataty, {'Inference-Header-Content-Length': 140}, None)> failed with timeout\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'InferAsyncRequest' object has no attribute 'as_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     28\u001b[0m input_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUPDATE 1-Five people in Hungary monitored for suspected anthrax infection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 29\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m send_request_to_bert_classifier(input_texts)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m, in \u001b[0;36msend_request_to_bert_classifier\u001b[1;34m(input_string)\u001b[0m\n\u001b[0;32m     18\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39masync_infer(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m, inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Process the output\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m output_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mas_numpy(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOUTPUT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Assuming the output is also in bytes and needs decoding\u001b[39;00m\n\u001b[0;32m     23\u001b[0m decoded_output \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m output_data\u001b[38;5;241m.\u001b[39mflatten()]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'InferAsyncRequest' object has no attribute 'as_numpy'"
     ]
    }
   ],
   "source": [
    "async def send_request_to_bert_classifier(input_string):\n",
    "    # Assuming input_strings is a list of strings\n",
    "    # Create a Triton client instance\n",
    "    client = httpclient.InferenceServerClient(url=\"localhost:8000\")\n",
    "\n",
    "    # Prepare the input data\n",
    "    numpy_input = np.array([bytes(input_string, encoding='utf-8')])\n",
    "\n",
    "    # Define the input tensor\n",
    "    inputs = [\n",
    "        httpclient.InferInput('CONTENT', numpy_input.shape, \"BYTES\")\n",
    "    ]\n",
    "\n",
    "    # Set the data for the input tensor\n",
    "    inputs[0].set_data_from_numpy(numpy_input, binary_data=True)\n",
    "\n",
    "    # Send inference request\n",
    "    response = client.async_infer(model_name=\"bert_classifier\", inputs=inputs)\n",
    "\n",
    "    # Process the output\n",
    "    output_data = response.as_numpy('OUTPUT')\n",
    "    # Assuming the output is also in bytes and needs decoding\n",
    "    decoded_output = [output.decode('utf-8') for output in output_data.flatten()]\n",
    "\n",
    "    return decoded_output\n",
    "\n",
    "# Example usage\n",
    "input_texts = \"UPDATE 1-Five people in Hungary monitored for suspected anthrax infection\"\n",
    "output = await send_request_to_bert_classifier(input_texts)\n",
    "print(\"Inference output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
